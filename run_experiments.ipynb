{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loguniform(low=0, high=1):\n",
    "    val = np.exp(np.random.uniform(0, 1, None))\n",
    "    scaled_val = (((val - np.exp(0)) * (high - low)) / (np.exp(1) - np.exp(0))) + low\n",
    "    return scaled_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def uniform(low=0, high=1):\n",
    "    val = np.random.uniform(low, high, None)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loguniform_int(low=0, high=1):\n",
    "    val = np.exp(np.random.uniform(0, 1, None))\n",
    "    scaled_val = (((val - np.exp(0)) * (high - low)) / (np.exp(1) - np.exp(0))) + low\n",
    "    return int(scaled_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_layers_values = [1, 2, 3]\n",
    "batch_size_values = [8, 16, 32, 64]\n",
    "optimizer_values = [\"rmsprop\", \"adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstmsize = loguniform_int(10, 150)\n",
    "dropout = uniform(0, 0.3)\n",
    "n_layers = n_layers_values[np.random.randint(0, len(n_layers_values))]\n",
    "batch_size = batch_size_values[np.random.randint(0, len(batch_size_values))]\n",
    "optimizer = optimizer_values[np.random.randint(0, len(optimizer_values))]\n",
    "learning_rate = loguniform(low=0.0001, high=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstmsize = 43,\n",
      "dropout = 0.184432748903,\n",
      "n_layers = 2,\n",
      "batch_size = 8,\n",
      "optimizer = adam,\n",
      "learning_rate = 0.00135703810542\n"
     ]
    }
   ],
   "source": [
    "print(\"lstmsize = %s,\\ndropout = %s,\\nn_layers = %s,\\nbatch_size = %s,\\noptimizer = %s,\\nlearning_rate = %s\" % (lstmsize, dropout, n_layers, batch_size, optimizer, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstmsize = 50\n",
    "dropout = 0\n",
    "n_layers = 1\n",
    "batch_size = 16\n",
    "learning_rate = 0.0005\n",
    "activation = \"sigmoid\"\n",
    "optimizer = \"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = \"bpic2017_accepted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset == \"minit\" or \"sample30000\" in dataset:\n",
    "    memory = 20000\n",
    "elif \"bpic2017\" in dataset or \"hospital_billing\" in dataset or \"traffic_fines\":\n",
    "    memory = 30000\n",
    "else:\n",
    "    memory = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstmsize = loguniform_int(10, 150)\n",
    "dropout = uniform(0, 0.3)\n",
    "n_layers = n_layers_values[np.random.randint(0, len(n_layers_values))]\n",
    "batch_size = batch_size_values[np.random.randint(0, len(batch_size_values))]\n",
    "optimizer = optimizer_values[np.random.randint(0, len(optimizer_values))]\n",
    "learning_rate = loguniform(low=0.000001, high=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lstmsize = np.random.randint(10, 31)\n",
    "#dropout = [0, 0.1, 0.2, 0.3, 0.4, 0.5][np.random.randint(0, 6)]\n",
    "#n_layers = 1\n",
    "#batch_size = batch_size_values[np.random.randint(0, len(batch_size_values))]\n",
    "#optimizer = optimizer_values[np.random.randint(0, len(optimizer_values))]\n",
    "#learning_rate = loguniform(low=0.0000001, high=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstmsize = 35,\n",
      "dropout = 0.0357106665075,\n",
      "n_layers = 2,\n",
      "batch_size = 8,\n",
      "optimizer = rmsprop,\n",
      "learning_rate = 4.66725606246e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"lstmsize = %s,\\ndropout = %s,\\nn_layers = %s,\\nbatch_size = %s,\\noptimizer = %s,\\nlearning_rate = %s\" % (lstmsize, dropout, n_layers, batch_size, optimizer, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results_dir = \"results/val_stability\"\n",
    "#n_iter = 5\n",
    "\n",
    "experiments_filename = \"validate_singletask_with_outcome_all_data.py\"\n",
    "#experiments_filename = \"validate_singletask_with_outcome_all_data_cv.py\"\n",
    "#experiments_filename = \"validate_singletask_with_outcome_all_data_stability.py\"\n",
    "\n",
    "method_name = \"validate_singletask_all_data\"\n",
    "#method_name = \"cv_validate_singletask_all_data\"\n",
    "#method_name = \"validate_singletask_all_data_stability\"\n",
    "\n",
    "params = \"lstmsize%s_dropout%s_nlayers%s_batchsize%s_lr%s_activation%s_optimizer%s\" % (lstmsize, dropout, n_layers, batch_size, learning_rate, activation, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x38b61d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"run.sh\", \"w\") as fout:\n",
    "    fout.write(\"#!/bin/bash\\n\")\n",
    "    fout.write(\"#SBATCH --partition=gpu\\n\")\n",
    "    fout.write(\"#SBATCH --gres=gpu:1\\n\")\n",
    "    fout.write(\"#SBATCH --output=results/output_files/val2_%s_%s_%s.csv\\n\" % (method_name, dataset, params))\n",
    "    fout.write(\"#SBATCH --mem=%s\\n\" % memory)\n",
    "\n",
    "    fout.write(\"python %s %s %s %s %s %s %s %s %s\" % (experiments_filename, dataset, lstmsize, dropout, n_layers,\n",
    "                                                     batch_size, learning_rate, activation, optimizer))\n",
    "\n",
    "time.sleep(5)\n",
    "subprocess.Popen(\"sbatch run.sh\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/evaluate final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = \"hospital_billing_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset == \"minit\" or \"sample30000\" in dataset:\n",
    "    memory = 20000\n",
    "elif \"bpic2017\" in dataset or \"hospital_billing\" in dataset or \"traffic_fines\":\n",
    "    memory = 30000\n",
    "else:\n",
    "    memory = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_params_filename = \"optimal_params.pickle\"\n",
    "embedding_dim = 30\n",
    "embedding_type = \"skipgram\"\n",
    "scale_model = \"row\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#experiments_filename = \"train_final.py\"\n",
    "experiments_filename = \"evaluate_final.py\"\n",
    "\n",
    "#method_name = \"train_final\"\n",
    "method_name = \"evaluate_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-60f4d5d1e38b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#SBATCH --partition=gpu\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#fout.write(\"#SBATCH --gres=gpu:1\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#SBATCH --output=results/output_files/emb_%s_%s_%s_%s.csv\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membedding_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#SBATCH --mem=%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_type' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"run.sh\", \"w\") as fout:\n",
    "    fout.write(\"#!/bin/bash\\n\")\n",
    "    fout.write(\"#SBATCH --partition=gpu\\n\")\n",
    "    #fout.write(\"#SBATCH --gres=gpu:1\\n\")\n",
    "    fout.write(\"#SBATCH --output=results/output_files/emb_%s_%s_%s_%s.csv\\n\" % (embedding_type, embedding_dim, method_name, dataset))\n",
    "    fout.write(\"#SBATCH --mem=%s\\n\" % memory)\n",
    "\n",
    "    fout.write(\"python %s %s %s %s %s %s\" % (experiments_filename, dataset, optimal_params_filename, embedding_type, embedding_dim, scale_model))\n",
    "    \n",
    "time.sleep(5)\n",
    "subprocess.Popen(\"sbatch run.sh\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = \"bpic2017_refused\"\n",
    "\n",
    "embedding_type = \"act_res_matrix\"\n",
    "embedding_dim = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dataset == \"minit\" or \"sample30000\" in dataset:\n",
    "    memory = 20000\n",
    "elif \"bpic2017\" in dataset or \"hospital_billing\" in dataset or \"traffic_fines\":\n",
    "    memory = 30000\n",
    "else:\n",
    "    memory = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x3118810>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"run.sh\", \"w\") as fout:\n",
    "    fout.write(\"#!/bin/bash\\n\")\n",
    "    fout.write(\"#SBATCH --partition=gpu\\n\")\n",
    "    fout.write(\"#SBATCH --gres=gpu:1\\n\")\n",
    "    fout.write(\"#SBATCH --output=results/output_files/val_%s_%s_%s.csv\\n\" % (dataset, embedding_type, embedding_dim))\n",
    "    fout.write(\"#SBATCH --mem=%s\\n\" % memory)\n",
    "\n",
    "    fout.write(\"python validate.py %s %s %s\" % (dataset, embedding_type, embedding_dim))\n",
    "    \n",
    "time.sleep(5)\n",
    "subprocess.Popen(\"sbatch run.sh\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
